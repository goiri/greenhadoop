// Scheduling in Hadoop based on green energy availability, brown energy cost, and peak brown power cost
//
// Assumptions and operation: Jobs are MapReduce computations; they do *not* specify number of nodes, 
// deadlines, or estimated run times.  Instead, we compute average running times and energy
// consumptions based on prior history.  These averages are used to estimate the resource
// requirements of groups of jobs, not each individual job.  Specifically, we first compute the
// average energy consumed by each job.  The energy consumed by the group of jobs is simply this
// average times the number of jobs in the group.  The total time the group of jobs will take is 
// the group's energy divided by the number of compute nodes we assign to the group.  (The reason 
// this should work is that the "static" energy -- S3, switch, always-on nodes energy -- is always 
// consumed in our system; everything else is dynamic energy that can be consumed at any rate.)
// The number of nodes depends on the load and whether there are peak brown power charges.
//
// In Hadoop, jobs can be submitted in five categories: very high priority, high priority, normal
// priority, low priority, and very low priority.  Very high priority jobs get as many nodes they 
// have tasks for (up to the cluster size, of course) and run as soon as possible; high priority 
// jobs get as many nodes they have tasks for up to half the size of the cluster, and run as soon 
// as possible; normal, low, and very low priority jobs get as few nodes as possible, making sure 
// that all jobs are expected to complete within 1 day.  These jobs are delayed to take advantage 
// of green energy and/or cheap brown electricity.
//
// Nodes is our system can be in one of three states: UP, DECOMMISSIONED, and DOWN (S3).
// Whenever nodes are not needed for computation or data access, we first decommission
// them and then send them to S3 state.  To create idle time for these nodes, we replicate
// any data the queued jobs need from them before sending them to S3.  Jobs can access as
// many input files as they desire.  For simplicity and to reduce the amount of replication, 
// we assume that each file needed by the jobs occupies a single HDFS block.  Every so often, 
// we reduce the amount of replication.
//
// Our scheduler uses 2 queues: Hadoop's own queue (with its priorities) and a waiting queue.
// Arriving very high and high priority jobs are sent directly to the Hadoop queue for 
// scheduling.  Normal, low, and very low priority jobs that only require data from the nodes that 
// are currently active are also sent directly to the Hadoop queue.  The other arriving normal, low,
// and very low priority jobs are kept in the waiting queue in FIFO order, until (1) there is 
// surplus green energy to activate the nodes they need for data, (2) delaying them longer would
// risk completing them after 1 day (this is done with internal deadlines), (3) the Hadoop queue 
// is empty, or (4) there are active nodes that are not fully utilized.
//
// Net metering (i.e., pumping green energy into the grid) is not used.  There can be many 
// reasons for that: (1) utility does not allow it; (2) utility pays less than the retail 
// electricity price for green energy pumped in; (3) there are energy losses when pumping 
// green energy into the grid; and (4) utility may not be able to use the energy pumped in.

// Submit a job to Green Hadoop.  Select queue based on priority and availability of data.
submitJob(job):
	// If the job's priority is above NORMAL or all the data it requires is available
	// on nodes that are UP, we go ahead and send it to Hadoop
	if (priority == VERY_HIGH or HIGH) OR isAvailable(job.requiredData()):
		submitJobHadoop(job)
	else:
		// Otherwise, we add it to our waiting queue
		addWaitingQueue(job)

// Submit a job to Hadoop
submitJobHadoop(job):
	bring up the minimum number of nodes containing job.requiredData()
	submit job to Hadoop

// Main cycle: Periodically, determine if we need to change the number of nodes that are UP.
// If so, replicate data and transition node states (if necessary).
every NODE_MANAGEMENT_PERIOD seconds or job finishes or job submitted or ...:
	reqNodes = calculateNodesOn()
	dispatch(reqNodes)

// Managing the waiting queue: Periodically, move jobs from 
// the waiting queue to Hadoop if (1) their internal deadlines have been reached, (2) the
// data they need is available, (3) there are nodes that are not fully utilized, or (4) 
// there is surplus green energy to turn on the nodes they need for data.
every CHECK_WAITING_QUEUE_PERIOD seconds:
	// Check the time limit and the data availability
	for job in waitingQueue:
	    if job.deadline < now OR isAvailable(job.requiredData()):
	       submitJobHadoop(job)
	       remove job from the waiting queue

	// Check if there are underutilized nodes
	while some nodes are not fully utilized:
	      submitJobHadoop(job)
	      remove job from the waiting queue

	// No need to check for surplus green energy explicitly, since it causes more nodes 
	// to be added.  Those added nodes would be initially free, so the utilization 
	// condition (above) applies

// Cleaning phase: Periodically, reduce replication (of most files) back to the default.
every CLEAN_HDFS_PERIOD seconds or enough green energy is available:
	Activate all nodes
	Set default replication level to all files not required by the queues

// Calculate the number of nodes required
calculateNodesOn():
	maxTime = TIME_WINDOW_SIZE_IN_SLOTS // number of NODE_MANAGEMENT_PERIOD slots in 1 day

	// Calculate the energy required by *all* the jobs using average_energy_job
	// We calculate this average energy from previous executions
	// average_energy_job = DYNAMIC_ENERGY/#jobs
	// DYNAMIC_ENERGY = everything that is not S3 energy
	queuesEnergy = average_energy_job * jobsInQueues

	// Compute/predict the energy available for each type of energy
	// greenEnergyAvail, cheapBrownEnergyAvail, and expenBrownEnergyAvail are arrays 
	// (one entry per time slot).  The latter two assume that all cluster nodes are up
	greenEnergyAvail = Predict the green energy to be available over maxTime
	cheapBrownEnergyAvail = Compute the cheap brown energy available over maxTime
	expenBrownEnergyAvail = Compute the expensive brown energy available over maxTime

	maxPower = Maximum power required by the cluster
	currPeakPower = Peak power until the moment

	// Subtract static power (S3, switch, and always-on nodes) from greenEnergyAvail, 
	// cheapBrownEnergyAvail, and expenBrownEnergyAvail
	update greenEnergyAvail, cheapBrownEnergyAvail, expenBrownEnergyAvail

	// Assign the green energy by subtracting queuesEnergy from greenEnergyAvail, until
	// queuesEnergy is all assigned or we run out of green energy
	for i = 0; i < maxTime && queuesEnergy > 0; i++
	    if greenEnergyAvail[i] > queuesEnergy:
	       greenEnergyAvail[i] -= queuesEnergy
               queuesEnergy = 0
        else:
	       queuesEnergy -= greenEnergyAvail[i]
	       greenEnergyAvail[i] = 0

	// If there is still queuesEnergy to be assigned, assign it to brown energy
	if queuesEnergy > 0:
		// Calculate optimal number of nodes for the periods of cheap and expensive 
		// brown electricity.  If peak brown charges are not in effect, use all nodes
		// during both periods.  Otherwise, search for the best number of nodes 
		// between the current peak power and the maximum peak power (all nodes).
		schedMaxPeak = calculateBrownShare(maxPower, queuesEnergy, 
				  cheapBrownEnergyAvail, expenBrownEnergyAvail)

	 	if peakCost == 0:
		   scheduling = schedMaxPeak
		else:
		   schedCurrPeak = calculateBrownShare(currPeakPower, queuesEnergy, 
				     cheapBrownEnergyAvail, expenBrownEnergyAvail)

		   if schedCurrPeak.toQueue == 0 AND schedCurrPeak.cost < schedMaxPeak.cost
			scheduling = schedCurrPeak
			minCost = schedCurrPeak.cost
		   else:
			// There's still some queuesEnergy to assign OR current peak == max peak
			scheduling = schedMaxPeak
			minCost = schedMaxPeak.cost

		   // Optimize (cost) brown vs expensive share by doing a binary search between
		   // the current number of nodes and the maximum number of nodes
		   maxPeak = maxPower
		   minPeak = currPeakPower

		   while minPeak <> maxPeak and ...:
			// Calculate peak power in the middle
			testPeak = (minPeak+maxPeak)/2
			testSched = calculateBrownShare(testPeak, queuesEnergy, 
				       cheapBrownEnergyAvail, expenBrownEnergyAvail)

			if testSched.toQueue > 0:
				minPeak = testPeak
			else:
				if testSched.cost < minCost:
					maxPeak = testPeak
				else:
					minPeak = testPeak

			// Check if this share is better than the previous
			if testSched.toQueue == 0 AND testSched.cost < minCost:
				scheduling = testSched
				minCost = testSched.cost

		// If scheduling.toQueue is still greater than 0, we need to reject new jobs
		// during the next NODE_MANAGEMENT_PERIOD iteration

	// Assign internal deadlines.  Each waiting job gets a deadline that is based on the 
	// overall completion deadline (1 day) and the expected duration of the group 
	// of jobs after (and including) it in the waiting queue 
	for job in waitingQueue:
	    if job.deadline == NULL:
	       assign internal deadline 

	// Calculate required nodes for "scheduling" right now (slot[0])
	onNodes = (scheduling.greenEnergyUsed[0] + scheduling.cheapBrownEnergyUsed[0] +
		  scheduling.expenBrownEnergyUsed[0]) / (POWER_AVERAGE - POWER_S3)

	return onNodes

// Helper function to calculate the best share of brown energy
calculateBrownShare(peak, queuesEnergy, brownCheap, brownExpensive):
	// Assign cheap, i.e. subtract queuesEnergy from brownCheap at a rate of peak power
	// during each slot until all queuesEnergy is assigned or brownCheap is over
	assign queuesEnergy to brownCheap with a maximum of peak
	
	if queuesEnergy > 0
		// Assign expensive, i.e. subtract queuesEnergy from brownExpensive at a 
		// rate of peak power during each slot until all queuesEnergy is assigned
		// or brownExpensive is over
		assign remaining queuesEnergy to brownExpensive with a maximum of peak

	// The brownScheduling variable contains many fields:
	//  toQueue: Energy remaining to assign if there is not enough available energy
	//  cost: Cost of this allocation
	//  greenEnergyUsed, cheapBrownEnergyUsed, expenBrownEnergyUsed: arrays listing
	//		     the amount of energy of each type that is to be used over time
	return brownScheduling

// Transition node states
dispatch(reqNodes):
	// Make sure that all required data for jobs in the Hadoop queue is available
	Turn into DEC state the min set of DOWN nodes containing data required by the Hadoop queue

	// If we need more nodes than those that are currently UP
	if reqNodes > UP nodes:
		// Start recommissioning nodes
		DEC->UP Recommission nodes:
			// We select nodes to recommission based on the following criteria
			HDFS:      Most data required
			MapReduce: Executed more tasks of still running jobs
		update reqNodes

	// If we still need more nodes to be UP, we start getting them back from S3
	if reqNodes > UP nodes:
		DOWN->UP Turn on S3 nodes:
			// We select nodes to activate based on the following criterion
			HDFS:      Containing more required data
		update reqNodes
	
	// If we need fewer nodes than the current set of active nodes
	if reqNodes < UP Nodes:
		// Start sending nodes to S3 by decommissioning them first
		UP->DEC Decommission nodes:
			// We select nodes to decommission based on the following criteria
			HDFS:      Less data required in the future
			MapReduce: Executed fewer tasks of still running jobs

		// Replicate the data before node goes to S3
		Replicate data from this node if (1) jobs in the Hadoop queue require it;
			  AND (2) the data is not already in an UP node
		update reqNodes
	
	// Check if the nodes that are in decommission state are ready to be sent to S3
	DEC->DOWN Turn off decommission nodes:
		// To be sent to S3, each decommissioned node must fulfill:
		HDFS:      All its required data is available in UP nodes
		MapReduce: Hasn't executed tasks of running jobs
